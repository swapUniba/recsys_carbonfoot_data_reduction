{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import typing as t\n",
    "import numpy as np\n",
    "\n",
    "class PairedTTest:\n",
    "    @staticmethod\n",
    "    def common_users(arr_0: t.Dict[int, float], arr_1:  t.Dict[int, float]):\n",
    "        return list(arr_0.keys() & arr_1.keys())\n",
    "\n",
    "    @staticmethod\n",
    "    def compare(arr_0: t.Dict[int, float], arr_1:  t.Dict[int, float]):\n",
    "        common_users = PairedTTest.common_users(arr_0, arr_1)\n",
    "        list_0 = list(map(arr_0.get, common_users))\n",
    "        list_1 = list(map(arr_1.get, common_users))\n",
    "        return stats.ttest_rel(list_0, list_1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_avg_recall(recall_at_k):\n",
    "\n",
    "    rec = 0\n",
    "    for user in recall_at_k:\n",
    "        rec += recall_at_k[user]\n",
    "    return rec / len(recall_at_k)\n",
    "\n",
    "def compute_recall_at_k(ground_truth_path, predictions_path, k=10):\n",
    "    \n",
    "    ground_truth = pd.read_csv(ground_truth_path, sep='\\t', names=['user', 'item', 'rating'])\n",
    "    predictions = pd.read_csv(predictions_path, sep='\\t', names=['user', 'item'])\n",
    "    \n",
    "    \n",
    "    ground_truth_dict = defaultdict(set)\n",
    "    for _, row in ground_truth.iterrows():\n",
    "        if int(row['rating']) >= 3:\n",
    "            ground_truth_dict[row['user']].add(row['item'])\n",
    "    \n",
    "    predictions_dict = defaultdict(list)\n",
    "    for _, row in predictions.iterrows():\n",
    "        predictions_dict[row['user']].append((row['item']))\n",
    "    \n",
    "    recall_at_k_dict = {}\n",
    "    \n",
    "    \n",
    "    for user, true_items in ground_truth_dict.items():\n",
    "        if user in predictions_dict:\n",
    "            \n",
    "            # no needed since RecBole return predictions that are already sorted\n",
    "            top_k_predictions = predictions_dict[user][0:k]\n",
    "            top_k_items = {item for item in top_k_predictions}\n",
    "            \n",
    "            \n",
    "            hits = len(true_items.intersection(top_k_items))\n",
    "            recall_at_k = hits / len(true_items)\n",
    "        else:\n",
    "            \n",
    "            recall_at_k = 0.0\n",
    "        \n",
    "        recall_at_k_dict[user] = recall_at_k\n",
    "    \n",
    "    return recall_at_k_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_avgpop(pop_at_k):\n",
    "    return sum(pop_at_k.values()) / len(pop_at_k)\n",
    "\n",
    "def compute_avgpop_at_k(ground_truth_path, predictions_path, k=10):\n",
    "    \n",
    "    ground_truth = pd.read_csv(ground_truth_path, sep='\\t', names=['user', 'item', 'rating'])\n",
    "    predictions = pd.read_csv(predictions_path, sep='\\t', names=['user', 'item'])\n",
    "    \n",
    "    \n",
    "    item_pop = ground_truth['item'].value_counts().to_dict()\n",
    "    \n",
    "    \n",
    "    predictions_dict = defaultdict(list)\n",
    "    pop_dict = defaultdict(float)\n",
    "    for _, row in predictions.iterrows():\n",
    "        user, item = row['user'], row['item']\n",
    "        if len(predictions_dict[user]) < k:\n",
    "            predictions_dict[user].append(item)\n",
    "            pop_dict[user] += item_pop.get(item, 0)\n",
    "    \n",
    "    \n",
    "    for user in pop_dict:\n",
    "        pop_dict[user] /= k\n",
    "\n",
    "    return pop_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gy/bpn4zxkd1nnf1160w51fjx880000gn/T/ipykernel_269/279748603.py:9: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ground_truth = pd.read_csv(ground_truth_path, sep='\\t', names=['user', 'item', 'rating'])\n",
      "/var/folders/gy/bpn4zxkd1nnf1160w51fjx880000gn/T/ipykernel_269/279748603.py:9: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ground_truth = pd.read_csv(ground_truth_path, sep='\\t', names=['user', 'item', 'rating'])\n",
      "/var/folders/gy/bpn4zxkd1nnf1160w51fjx880000gn/T/ipykernel_269/279748603.py:9: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ground_truth = pd.read_csv(ground_truth_path, sep='\\t', names=['user', 'item', 'rating'])\n",
      "/var/folders/gy/bpn4zxkd1nnf1160w51fjx880000gn/T/ipykernel_269/279748603.py:9: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ground_truth = pd.read_csv(ground_truth_path, sep='\\t', names=['user', 'item', 'rating'])\n",
      "/var/folders/gy/bpn4zxkd1nnf1160w51fjx880000gn/T/ipykernel_269/279748603.py:9: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ground_truth = pd.read_csv(ground_truth_path, sep='\\t', names=['user', 'item', 'rating'])\n",
      "/var/folders/gy/bpn4zxkd1nnf1160w51fjx880000gn/T/ipykernel_269/279748603.py:9: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ground_truth = pd.read_csv(ground_truth_path, sep='\\t', names=['user', 'item', 'rating'])\n"
     ]
    }
   ],
   "source": [
    "# recall\n",
    "rec_lgcn_2 = compute_recall_at_k('amazon_books_60core.test.tsv', 'preds/amazon_books_60core_split_2_LightGCN_preds.tsv', k=10)\n",
    "avg_rec_lgcn_2 = compute_avg_recall(rec_lgcn_2)\n",
    "\n",
    "rec_dmf_2 = compute_recall_at_k('amazon_books_60core.test.tsv', 'preds/amazon_books_60core_split_2_DMF_preds.tsv', k=10)\n",
    "avg_rec_dmf_2 = compute_avg_recall(rec_dmf_2)\n",
    "\n",
    "rec_ngcf_2 = compute_recall_at_k('amazon_books_60core.test.tsv', 'preds/amazon_books_60core_split_2_NGCF_preds.tsv', k=10)\n",
    "avg_rec_ngcf_2 = compute_avg_recall(rec_ngcf_2)\n",
    "\n",
    "rec_lgcn_6 = compute_recall_at_k('amazon_books_60core.test.tsv', 'preds/amazon_books_60core_split_6_LightGCN_preds.tsv', k=10)\n",
    "avg_rec_lgcn_6 = compute_avg_recall(rec_lgcn_6)\n",
    "\n",
    "rec_dmf_6 = compute_recall_at_k('amazon_books_60core.test.tsv', 'preds/amazon_books_60core_split_6_DMF_preds.tsv', k=10)\n",
    "avg_rec_dmf_6 = compute_avg_recall(rec_dmf_6)\n",
    "\n",
    "rec_ngcf_6 = compute_recall_at_k('amazon_books_60core.test.tsv', 'preds/amazon_books_60core_split_6_NGCF_preds.tsv', k=10)\n",
    "avg_rec_ngcf_6 = compute_avg_recall(rec_ngcf_6)\n",
    "\n",
    "rec_lgcn_10 = compute_recall_at_k('amazon_books_60core.test.tsv', 'preds/amazon_books_60core_split_10_LightGCN_preds.tsv', k=10)\n",
    "avg_rec_lgcn_10 = compute_avg_recall(rec_lgcn_10)\n",
    "\n",
    "rec_dmf_10 = compute_recall_at_k('amazon_books_60core.test.tsv', 'preds/amazon_books_60core_split_10_DMF_preds.tsv', k=10)\n",
    "avg_rec_dmf_10 = compute_avg_recall(rec_dmf_10)\n",
    "\n",
    "rec_ngcf_10 = compute_recall_at_k('amazon_books_60core.test.tsv', 'preds/amazon_books_60core_split_10_NGCF_preds.tsv', k=10)\n",
    "avg_rec_ngcf_10 = compute_avg_recall(rec_ngcf_10)\n",
    "\n",
    "\n",
    "# avg pop\n",
    "pop_lgcn_2 = compute_avgpop_at_k('amazon_books_60core_split_2.train.tsv', 'preds/amazon_books_60core_split_2_LightGCN_preds.tsv', k=10)\n",
    "avg_pop_lgcn_2 = compute_avgpop(pop_lgcn_2)\n",
    "\n",
    "pop_dmf_2 = compute_avgpop_at_k('amazon_books_60core_split_2.train.tsv', 'preds/amazon_books_60core_split_2_DMF_preds.tsv', k=10)\n",
    "avg_pop_dmf_2 = compute_avgpop(pop_dmf_2)\n",
    "\n",
    "pop_ngcf_2 = compute_avgpop_at_k('amazon_books_60core_split_2.train.tsv', 'preds/amazon_books_60core_split_2_NGCF_preds.tsv', k=10)\n",
    "avg_pop_ngcf_2 = compute_avgpop(pop_ngcf_2)\n",
    "\n",
    "pop_lgcn_6 = compute_avgpop_at_k('amazon_books_60core_split_6.train.tsv', 'preds/amazon_books_60core_split_6_LightGCN_preds.tsv', k=10)\n",
    "avg_pop_lgcn_6 = compute_avgpop(pop_lgcn_6)\n",
    "\n",
    "pop_dmf_6 = compute_avgpop_at_k('amazon_books_60core_split_6.train.tsv', 'preds/amazon_books_60core_split_6_DMF_preds.tsv', k=10)\n",
    "avg_pop_dmf_6 = compute_avgpop(pop_dmf_6)\n",
    "\n",
    "pop_ngcf_6 = compute_avgpop_at_k('amazon_books_60core_split_6.train.tsv', 'preds/amazon_books_60core_split_6_NGCF_preds.tsv', k=10)\n",
    "avg_pop_ngcf_6 = compute_avgpop(pop_ngcf_6)\n",
    "\n",
    "pop_lgcn_10 = compute_avgpop_at_k('amazon_books_60core_split_10.train.tsv', 'preds/amazon_books_60core_split_10_LightGCN_preds.tsv', k=10)\n",
    "avg_pop_lgcn_10 = compute_avgpop(pop_lgcn_10)\n",
    "\n",
    "pop_dmf_10 = compute_avgpop_at_k('amazon_books_60core_split_10.train.tsv', 'preds/amazon_books_60core_split_10_DMF_preds.tsv', k=10)\n",
    "avg_pop_dmf_10 = compute_avgpop(pop_dmf_10)\n",
    "\n",
    "pop_ngcf_10 = compute_avgpop_at_k('amazon_books_60core_split_10.train.tsv', 'preds/amazon_books_60core_split_10_NGCF_preds.tsv', k=10)\n",
    "avg_pop_ngcf_10 = compute_avgpop(pop_ngcf_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# LGCN\n",
    "results['rec_lgcn_2_vs_rec_lgcn_6'] = PairedTTest.compare(rec_lgcn_2, rec_lgcn_6)\n",
    "results['rec_lgcn_2_vs_rec_lgcn_10'] = PairedTTest.compare(rec_lgcn_2, rec_lgcn_10)\n",
    "results['rec_lgcn_6_vs_rec_lgcn_10'] = PairedTTest.compare(rec_lgcn_6, rec_lgcn_10)\n",
    "results['pop_lgcn_2_vs_pop_lgcn_6'] = PairedTTest.compare(pop_lgcn_2, pop_lgcn_6)\n",
    "results['pop_lgcn_2_vs_pop_lgcn_10'] = PairedTTest.compare(pop_lgcn_2, pop_lgcn_10)\n",
    "results['pop_lgcn_6_vs_pop_lgcn_10'] = PairedTTest.compare(pop_lgcn_6, pop_lgcn_10)\n",
    "\n",
    "# DMF\n",
    "results['rec_dmf_2_vs_rec_dmf_6'] = PairedTTest.compare(rec_dmf_2, rec_dmf_6)\n",
    "results['rec_dmf_2_vs_rec_dmf_10'] = PairedTTest.compare(rec_dmf_2, rec_dmf_10)\n",
    "results['rec_dmf_6_vs_rec_dmf_10'] = PairedTTest.compare(rec_dmf_6, rec_dmf_10)\n",
    "results['pop_dmf_2_vs_pop_dmf_6'] = PairedTTest.compare(pop_dmf_2, pop_dmf_6)\n",
    "results['pop_dmf_2_vs_pop_dmf_10'] = PairedTTest.compare(pop_dmf_2, pop_dmf_10)\n",
    "results['pop_dmf_6_vs_pop_dmf_10'] = PairedTTest.compare(pop_dmf_6, pop_dmf_10)\n",
    "\n",
    "# NGCF\n",
    "results['rec_ngcf_2_vs_rec_ngcf_6'] = PairedTTest.compare(rec_ngcf_2, rec_ngcf_6)\n",
    "results['rec_ngcf_2_vs_rec_ngcf_10'] = PairedTTest.compare(rec_ngcf_2, rec_ngcf_10)\n",
    "results['rec_ngcf_6_vs_rec_ngcf_10'] = PairedTTest.compare(rec_ngcf_6, rec_ngcf_10)\n",
    "results['pop_ngcf_2_vs_pop_ngcf_6'] = PairedTTest.compare(pop_ngcf_2, pop_ngcf_6)\n",
    "results['pop_ngcf_2_vs_pop_ngcf_10'] = PairedTTest.compare(pop_ngcf_2, pop_ngcf_10)\n",
    "results['pop_ngcf_6_vs_pop_ngcf_10'] = PairedTTest.compare(pop_ngcf_6, pop_ngcf_10)\n",
    "\n",
    "# Compare same dataset, different models\n",
    "datasets = [2, 6, 10]\n",
    "\n",
    "for dataset in datasets:\n",
    "    results[f'rec_lgcn_{dataset}_vs_rec_dmf_{dataset}'] = PairedTTest.compare(globals()[f'rec_lgcn_{dataset}'], globals()[f'rec_dmf_{dataset}'])\n",
    "    results[f'rec_lgcn_{dataset}_vs_rec_ngcf_{dataset}'] = PairedTTest.compare(globals()[f'rec_lgcn_{dataset}'], globals()[f'rec_ngcf_{dataset}'])\n",
    "    results[f'rec_dmf_{dataset}_vs_rec_ngcf_{dataset}'] = PairedTTest.compare(globals()[f'rec_dmf_{dataset}'], globals()[f'rec_ngcf_{dataset}'])\n",
    "    results[f'pop_lgcn_{dataset}_vs_pop_dmf_{dataset}'] = PairedTTest.compare(globals()[f'pop_lgcn_{dataset}'], globals()[f'pop_dmf_{dataset}'])\n",
    "    results[f'pop_lgcn_{dataset}_vs_pop_ngcf_{dataset}'] = PairedTTest.compare(globals()[f'pop_lgcn_{dataset}'], globals()[f'pop_ngcf_{dataset}'])\n",
    "    results[f'pop_dmf_{dataset}_vs_pop_ngcf_{dataset}'] = PairedTTest.compare(globals()[f'pop_dmf_{dataset}'], globals()[f'pop_ngcf_{dataset}'])\n",
    "\n",
    "# print results when not statistically significant\n",
    "for key, p_value in results.items():\n",
    "    if p_value >= 0.05:\n",
    "        print(f'{key}: {p_value} --> not a statistically significant difference')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "with open('ttest_results.tsv', 'w') as fout:\n",
    "    for key, p_value in results.items():\n",
    "        if p_value < 0.05:\n",
    "            out_msg = 'statistically significant difference'\n",
    "        else:\n",
    "            out_msg = 'not a statistically significant difference'\n",
    "        fout.write(f'{key}:\\t{p_value}\\t{out_msg}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
